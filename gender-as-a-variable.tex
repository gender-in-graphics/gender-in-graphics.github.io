\PassOptionsToPackage{table}{xcolor}
\documentclass[nonacm,sigconf,review,balance=false]{acmart}
\setcopyright{none}
\citestyle{acmauthoryear}
\setcitestyle{square}

\setcopyright{none}
\acmConference[Under Review]{Under Review}{Under Review}{Online} \acmYear{2021}
\acmDOI{nnn.nnn}

\settopmatter{authorsperrow=4}
\input{common-preamble}

\begin{document}

\title{Gender and Sex in the Computer Graphics research literature}

\author{Ana Dodik}\authornote{Joint First Authors}
\affiliation{\institution{Meta Platforms}}

\author{Silvia Sell√°n}\authornotemark[1]
\affiliation{\institution{University of Toronto}}


\author{Theodore Kim}
\affiliation{\institution{Yale University}}

\author{Amanda Phillips}
\affiliation{\institution{Georgetown University}}


\begin{abstract}
    We survey the treatment of sex and gender in the Computer Graphics research
    literature and its scientific and real-world consequences. We conclude
    current trends on the use of gender in our research community are scientifically incorrect and constitute a
    form of algorithmic bias with harmful effects. We propose ways for
    correcting these trends and pose novel research questions.
\end{abstract}


\maketitle

%\ana{I understand wanting to focus on SIGGRAPH, but I think we should consider broadening the scope to include all of visual computing research. Computer vision is \textit{especially} guilty of this.}

\section{Introduction}

References to sex and gender can be found all throughout the Computer Graphics research literature: a dataset is said to contain images of men and women, user study participants are reported to have a certain male/female ratio, a body modeling algorithm trains two different gendered models, a voice modification method is said to work on male and female voices, etc. 

The scientific consensus around the concepts of sex and gender has greatly evolved in the past decades (see, e.g., \cite{pmid30377332}). As surveyed by \citet{fausto2012sex}, \emph{sex} is not one but a combination of many biological classifications (\emph{chromosomal sex, hormonal sex, reproductive sex, ...}) which cannot be unambiguously assigned in a binary way to as much as one in 50 people \cite{blackless2000sexually}. 
\emph{Gender}, on the other hand, is used to refer to an individual's self-identity \cite{money1972man}, their performance of certain acts \cite{butler2003gender} or arbitrary social organizational structures that segregate people in different public bathrooms and even decide who can access education or participate in public life \cite{lorber1994paradoxes}. By all these contemporary definitions, gender is non-binary, fluid and culturally-specific. Furthermore, assuming outdated binary definitions of sex and gender is not just scientifically incorrect, but can also be shown to be harmful to those who conform the least to this artificial binary \cite{un2015report}.

Despite this, we observe that the treatment of sex and gender in Computer Graphics research
still answers to a traditional binary understanding of it that excludes intersex and many transgender and
gender non-conforming people. We argue that our community's current use of gender is
imprecise, contradictory and detrimental to our scientific integrity.
We examine the harmful real-world consequences of the algorithmic bias introduced by our modeling
choices with respect to gender on how gender non-conforming people interact with
our technology in their daily lives. We advocate for reexamining our
treatment of gender and show that this will not only correct worrying trends in
our community, but also open the door to whole new avenues of research.

\section{Survey}

%\ana{So a small suggestion on how to make this more formal, we could indicate which type of algorithmic unfairness the different papers introduce, e.g.~\cite{FriedmanAndNissenbaum}.}

Inspired by the work of \citet{keyes2018misgendering}, we conducted a survey of
all technical papers presented at SIGGRAPH North America and SIGGRAPH Asia since
2015 (see supplemental material). We observed references to gender routinely
throughout, varying in nature from demographic information reported about user
study participants to gender-specific algorithms. Whenever gender is used
explicitly as a variable, it is always as a binary one. Despite its prominence,
gender is never given a precise definition in all the reviewed Computer Graphics
literature, and appears to be used implicitly as a proxy for anything from body
proportions to facial expression to voice inflection in speech.
%\ana{100\% this. When it comes to body modeling, from an algorithmic stand-point, you could want to cluster based on commonly co-occuring body features, but (binary) gender is just a bad (socially-constructed) proxy for that. Also, proxies are a thing in algo.~fairness research \cite{BigDataImpact}. I am beig told that this is apparently similar to Judith Butler's view on binary bodies in Gender Trouble?}

An analysis of the above reveals worrying trends about the current use of gender
as a variable in Computer Graphics, both scientifically and ethically. As we
mention examples of works that perpetuate these trends, we stress that we do not
associate any malicious intent to any. Rather, we wish to show how seemingly
neutral, well-established practices in our community can lead to us unwittingly
perpetuating forms of algorithmic bias.

%\ana{There is something to be said about the entire ``It's just the data thats biased'' discussion we keep having in ML ad nauseam. In our examples, it's not just the data that's the root of algorithmic unfairness, it's the active decisions of algorithm designers. For practitioners this means that, even if you don't have access to, e.g.~body scans of non-binary people, you can still try to remove a part of that bias that is due to algorithm design decisions.}

\section{Algorithmic Fairness Analysis}

Our literature survey demonstrates that the current use of gender in the computer graphics literature is at best ill-defined, and at worst incorrect. In this Section, we demonstrate through various examples why such an approach causes a number of \emph{technical} issues within the surveyed works, and is, as such, at odds with producing precise and high-quality reproducible research.

In our discussion, we apply a framework which disambiguates and categorizes different types of bias according to the stages of a system's lifecycle \cite{Suresh2021}. Note that while the original paper focuses on machine learning, we find it to be equally applicable to general problems in computer graphics. We give concrete examples of how \emph{all} types of bias occur throughout the surveyed work.

% \subsection{Task Definition and Data Unfairness}

\paragraph*{Historical bias} Historical bias occurs when data encodes existing prejudice. For example, a \emph{gender classifier} trained on data collected in a society where social norms dictate gender expression might learn that ``wearing a dress'' means woman, and ``short hair'' means man, despite the data being \emph{abundant} and \emph{perfectly sampled}. \ana{Another possible example from Silvia: virtual garment try-on methods can merge a person's body proportions with their preference in attire. Not sure which is better to use?}

\paragraph*{Representation bias} This type of bias occurs when a part of a population is poorly represented by a dataset. This can happen due to a multitude of reasons:
\begin{itemize}
    \item A dataset can lead to algorithmic unfairness if it is uniformly sampled, but \emph{contains under-represented groups}. Reported numbers for the prevalence of gender-diverse individuals range from $0.1\%$ to $2\%$, with the percentage increasing in geographic areas where gender diversity is more accepted (\ana{cite something}). Therefore, a dataset with $1000$ people is expected include between $1$ and $20$ gender-diverse people. This means that the algorithms trained on this dataset are more likely to produce worse results on gender-diverse individuals.
    \item \emph{Sample selection bias} occurs when the sampling procedure is biased in such a way to not include any gender-diverse people. The datasets we reviewed should include trans and non-binary people based on the reported statistics about the prealence of such individuals. A possible explanation is that the sampling procedure was accidentally designed in such a way to to decrease the likelihood of capturing gender-diverse individuals, or it might also occur due to measurement bias.
\end{itemize}

% Measurement, or reporting, bias arises from how we choose, utilize,and measure particular features.
\paragraph*{Measurement bias} Measurement, or reporting bias is typically introduced during the task definition through the selection and measurement of features and target variables. During our literature review, we identified the presence of the following issues:
\begin{itemize}
    \item The frequent usage of \emph{proxies}, both as features and as target variables. For example, certain papers use \emph{sex} or \emph{gender} to mean \emph{comonly co-occuring bodily characteristics} or \emph{gender expression}.
    The use of \emph{sex} and \emph{gender} as proxy variables is not only harmful, but often also imprecise and inaccurate from a technical stand-point. It is quite possible that the authors would be better served using other less abstract features (e.g. hair length, or voice pitch). The usage of these variables can further lead to \emph{omitted variable bias}, which we discuss later in the article.
    \item An \emph{inaccurate} method of measurement. For example, all of the papers we reviewed treat gender as a \emph{discrete binary variable}. Therefore, even if data was collected from gender-diverse participants, the method of measurement would not be able to capture that.
    \item An \emph{incorrect} method of measurement. An example of this would be works which use data labelers to ``label'' people's gender from images, instead of letting the individuals self-identify. Again, algorithmic bias occurs in this situation, even if special care is taken to collect images of gender-diverse individuals.
\end{itemize}.

\paragraph*{Aggregation bias} \ana{I need help on this one, since I cannot think of any examples. From the survey paper: ``Aggregation bias (or ecological fallacy) arises when false conclusions are drawn about individuals from observing the entire population. An example of this type of bias can be seen in clinical aid tools. Consider diabetes patients who have apparent morbidity differences across ethnicities and genders.
Specifically, HbA1c levels, that are widely used to diagnose and monitor diabetes, differ in complex ways across genders and ethnicities. Therefore, a model that ignores individual differences will likely not be well-suited for all ethnic and gender groups in the population.
This is true even when they are represented equally in the training data. Any general assumptions about subgroups within the population can result in aggregation bias.''}

\paragraph*{Learning bias} Algorithm designers will often unknowingly introduce bias into their methods. For example, using a regularizer that makes sure to regress to an ``average'' body is likely to make the algorithm worse for people far away from the average. Alternatively, optimizing for a model's accuracy might lead to the statistical pairity decreasing accross different groups.

\paragraph*{Evaluation bias} Evaluation bias encompases all bias that is introduced during the evaluation of an algorithm. If the computer graphics community settles on benchmarks with biased data or metrics, it creates a knock-on effect where the development and deployment of models that performed well on those benchmarks is further encouraged. This is one of the reasons it is particularly dangerous to "leave the fairness questions" for future work.

\paragraph*{Deployment bias} Deployment bias refers to the harm that is introduced as a consequence of the model being published or deployed in the real world:
\begin{itemize}
    \item Assuming an implicit definition of (e.g. binary) gender might incentivize future researchers to conform to that definition. This is additionally problematic when researchers from cultures with a different understanding of gender need to adjust to foreign cultural norms. (\ana{Amanda, I might need some help with this one.})
    \item Deploying an algorithm can lead to \emph{feedback loops}. For example, if gender-diverse people have poor experiences with clothing size recommender systems, they are less likely to use them. As a consequence, the data about the performance of such a system will be skewed to include fewer gender-diverse people. Using such data to further optimize the systems can lead to compounding effects.
    \item A deployed system can cause harm by nudging the users to artificially change their behavior. For example, a trans person might feel the need to change the pitch of their voice in order to not get misgendered by an algorithm.
    \item Deploying a body model which has ``male'', ``female'', and ``gender neutral'' variants might lead to unintentional harm, if, for example, the ``gender neutral'' model is used only that if the algorithm does not have enough confidence the user is either ``male'' or ``female''. This can lead to gender trans or non-confirming individuals being actiely told that they are not recognized as the gender they identify with.
\end{itemize}

\paragraph*{Omitted variable bias} Omitted vairable bias occurs when the success of using a certain feature is overemphasized because it correlates with another important feature that has been omitted from the modeling stage. For example, ``gender'' is likely not as discriminative as a variable when the result is also conditioned on ``hair length'', or ``hip width''. Alternatively, a voice modification algorithm may attribute its performance to gender, rather than a combination of pitch and certain socially-acquired speech inflections. \ana{cite paper that introduces this concept because it's not \cite{Suresh2021}.}

\ana{I don't know where to fit this: a proposed new conversational agent might learn a correlation between a person's visual appearance and certain traits they exhibit in non-verbal communication. Suggestions?}

In general, we have found the gender-related language to be imprecise, which hinders the clarity of the presentation. Oftentimes, we found that \emph{gender} and \emph{sex} are used seemingly interchangebly, and most of the times it is not even clear from context which one the authors intended to use. In fact, we argue that it is impossible to know a-priori if a trained model has picked up on \emph{gender} characteristics, and not on the characteristics of \emph{sex} or \emph{gender expression}, without looking at algorithmic fairness metrics accross different subgroups.

% society tends to conflate gender expression and gender identity.

A worrying trend we noticed is that none of the reviewed papers provide an analysis of the algorithmic biases that they potentially introduce. While different real-world constraints might not make it realistic for a research group to successfully mitigate certain sources of bias, the potentially introduced biases should at the very least be acknowledged. For example, none of the reviewed papers included any of the various algorithmic fairness metrics (\ana{cite metrics}) into their evaluation, nor did they even include a discussion of the potential harm their methods could be causing.

Algorithmic fairness evaluations and discussions are left out of computer graphics papers not because they happen to be difficult or time-consuming, but because they are deemed unnecessary by the people who would at large be either unaffected or positively affected by the introduced biases.

However, we argue that the problems introduced in these methods are not only potentially harmful to under-represented populations, but also often \emph{technically limited ways which are not well researched}. If a method cannot model a class of humans by design, or if a production system fails for a subsection of the population, these are fundamental \emph{technical} limitations. The question is then, why does our community prioritize solving these limitations disproportionatly less then other technical problems?

\subsection{Real world harm}

\ana{Disambiguate between allocative and representational manifestation of harms, Barocas et al.}
\ana{The mental model for data collection should not be ``we need samples from $N$ people'', rather it should be ``we need samples from $m \times k \approx n$ people, such that each of the $k$ groups of $m$ people happen to be representative of a certain protected characteristic''.}
\ana{We argue that discussions around algorithmic fairness need to become front-and-center within our own community, instead of being relegated to other venues or "future work".}
\ana{People, measure your bias!}
\ana{IT IS NOT NECESSARY TO USE GENDER AS A VARIABLE.}

As scientific researchers, we must be aware of the effect that our arbitrary
modelling decisions have in the real world as our algorithms are used by
governments and private companies.

Since many people's gender experiences fall outside the male/female binary, our
research's insistence on it can contribute to frustration (at best) and
discrimination (at worst) when they interact with technology. A researcher's
seemingly inocuous decision to use different search spaces for fitting male and
female body proportions leads to airport body scanners that routinely subject
transgender passangers to humilliation (see \cite{tsa}). A modelling choice to
conflate body proportions with choices in attire ironically excludes precisely
the people with non-normative bodies who are the most in danger in traditional
physical changing rooms (see e.g., \cite{changingroom}). \ana{lol please explain this to my tech lead...}


These negative effects are compounded even further as our algorithms are being
used to generate synthetic datasets on which to train Machine Learning
algorithms outside of our research area. If we do not examine and properly
report our algorithm's limitations in representing people outside of the gender
binary, these can later be used to train autonomous vehicles to detect
pedestrians (\cite{cars}), medical diagnosing tools (\cite{chen2021synthetic})
and even security threat detection (\cite{dhs}).

Furthermore, as Computer Graphics researchers, we must consider our role in
shaping whose stories get to be told and who gets to seem themselves represented
in the entertainment culture. By conflating different attributes under the
umbrella of gender, we exclude gender non-conforming individuals from every
videogame and movie created using our tool, further invisibilizing
already-invisible and marginalized communities.

It bears mentioning that our research community's entrenchment in the
traditional gender binary is a rare example of Computer Graphics research
lagging behind the needs of our partner industries. \emph{Metahuman}, the latest
photorrealistic character modeller by \citet{metahuman} has no mention of
gender; \citet{googlegender} removed all gender references from its Cloud Vision
API; video games as diverse as \emph{Animal Crossing: New Horizons},
\emph{Cyberpunk 2077} and \emph{Forza Horizon 5} completely decouple attributes
like hairstyle, body proportions, voice pitch and prononouns from one another. \ana{I am \textit{very} against including Cyberpunk as a positive example of anything gender-related.}

Finally, the current use of gender in the Computer Graphics literature creates a
hostile environment for gender non-conforming members of our research community,
which goes against ACM SIGGRAPH's goal to be \emph{a model of inclusion, equity,
access and diversity for all}: \ana{I would end the sentence here. Also, the previous sentence might be good to include in the abstract or as one of the very first sentences.} by seeing colleagues and collaborators
consistently exclude us \ana{us $\rightarrow$ gender-diverse people} from their own research work, we are (willingly or not)
sent the message that we do not belong in this research community, encouraging
us to look for jobs elsewhere.

\section{Where do we go from here?}

We believe the reasons above to be enough to make us reevaluate the role of
gender in our community's scientific literature.

For example, the reporting of gender among other demographic information in user
study participants and dataset collection subjects answer to a scientifically
positive goal (experimental transparency) as well as an ethical one, to
safeguard against the ‚Äúmale default‚Äù that plagues science and has plagued it
since its infancy. However, we found instances in our survey of participants
being reported as of ‚Äúunknown gender‚Äù, which may indicate that their gender is
being assumed post facto by researchers as opposed to self reported, leading to
the potential misidentification and exclusion of gender non-conforming
individuals or of those from certain ethnicities (see e.g.,
\cite{santamaria2018comparison,buolamwini2018gender}). Therefore, we would argue
it is still advisable to include this kind of data, as long as it is self
reported by participants who are given a breadth of gender options not
restricted to the traditional binary ones.

On the other hand, the scientific and ethical harm caused by gender-segregated
algorithms is likely too significant to offset any possible benefits. At the
very least, these choices should be justified and their consequences in terms of
excluding gender non-conforming individuals should be examined and clearly
stated. Eventually, we hope that our field evolves to address these limitations
and move beyond the outdated gender binary. We trust that our fellow researchers
share our scientific excitement in this new frame of reference and the potential
novel research directions it opens; for example:
\begin{itemize}
    \item What is a complete parametric model for the human body that is
    decoupled from gender and accurately represents the diverse bodies of all
    humans, regardless of whether they conform to traditional gender norms?
    \item How can our research inform or contrast more modern understandings of
    gender? Can data-based methods be used to evaluate cultural differences in
    gender presentation?
    \item How can we evaluate our algorithms for bias towards the gender binary?
    What tools are needed to obtain or synthesize data that covers more diverse
    experiences of gender?
\end{itemize}

We acknowledge that our proposed break with tradition may bring with it effort
and difficult conversations, but these are challenges worth facing in the
interest of scientific advancement as well as producing a fairer, more inclusive
future.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

\end{document}
