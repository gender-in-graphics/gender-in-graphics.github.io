\PassOptionsToPackage{table}{xcolor}
\documentclass[nonacm,sigconf,review,balance=false]{acmart}
\setcopyright{none}
\citestyle{acmauthoryear}
\setcitestyle{square}

\setcopyright{none}
\acmConference[Under Review]{Under Review}{Under Review}{Online} \acmYear{2021}
\acmDOI{nnn.nnn}

\settopmatter{authorsperrow=4}
\input{common-preamble}

\begin{document}

\title{Gender and Sex in the Computer Graphics research literature}

\author{Ana Dodik}\authornote{Joint First Authors}
\affiliation{\institution{Meta Platforms}}

\author{Silvia Sell√°n}\authornotemark[1]
\affiliation{\institution{University of Toronto}}


\author{Theodore Kim}
\affiliation{\institution{Yale University}}

\author{Amanda Phillips}
\affiliation{\institution{Georgetown University}}


\begin{abstract}
    We survey the treatment of sex and gender in the Computer Graphics research
    literature and its scientific and real-world consequences. We conclude
    current trends on the use of gender in our research community are scientifically incorrect and constitute a
    form of algorithmic bias with harmful effects. We propose ways for
    correcting these trends and pose novel research questions.
\end{abstract}


\maketitle

%\ana{I understand wanting to focus on SIGGRAPH, but I think we should consider broadening the scope to include all of visual computing research. Computer vision is \textit{especially} guilty of this.}

\section{Introduction}

References to sex and gender can be found all throughout the Computer Graphics research literature: a dataset is said to contain images of men and women, user study participants are reported to have a certain male/female ratio, a body modeling algorithm trains two different gendered models, a voice modification method is said to work on male and female voices, etc.

The scientific consensus around the concepts of sex and gender has greatly evolved in the past decades (see, e.g., \cite{pmid30377332}). As surveyed by \citet{fausto2012sex}, \emph{sex} is not one but a combination of many biological classifications (\emph{chromosomal sex, hormonal sex, reproductive sex, ...}) which cannot be unambiguously assigned in a binary way to as much as one in 50 people \cite{blackless2000sexually}.
\emph{Gender}, on the other hand, is used to refer to an individual's self-identity \cite{money1972man}, their performance of certain acts \cite{butler2003gender} or arbitrary social organizational structures that segregate people in different public bathrooms and even decide who can access education or participate in public life \cite{lorber1994paradoxes}. By all these contemporary definitions, gender is non-binary, fluid and culturally-specific. Furthermore, assuming outdated binary definitions of sex and gender is not just scientifically incorrect, but can also be shown to be harmful to those who conform the least to this artificial binary \cite{un2015report}.

Despite this, we observe that the treatment of sex and gender in Computer Graphics research
still answers to a traditional binary understanding of it that excludes intersex and many transgender and
gender non-conforming people. We argue that our community's current use of gender is
imprecise, contradictory and detrimental to our scientific integrity.
We examine the harmful real-world consequences of the algorithmic bias introduced by our modeling
choices with respect to gender on how gender non-conforming people interact with
our technology in their daily lives. We advocate for reexamining our
treatment of gender and show that this will not only correct worrying trends in
our community, but also open the door to whole new avenues of research.

\section{Survey}

%\ana{So a small suggestion on how to make this more formal, we could indicate which type of algorithmic unfairness the different papers introduce, e.g.~\cite{FriedmanAndNissenbaum}.}

Inspired by the work of \citet{keyes2018misgendering}, we conducted a survey of
all technical papers presented at SIGGRAPH North America and SIGGRAPH Asia since
2015 (see supplemental material). We observed references to gender routinely
throughout, varying in nature from demographic information reported about user
study participants to gender-specific algorithms. Whenever gender is used
explicitly as a variable, it is always as a binary one. Despite its prominence,
gender is never given a precise definition in all the reviewed Computer Graphics
literature, and appears to be used implicitly as a proxy for anything from body
proportions to facial expression to voice inflection in speech.
%\ana{100\% this. When it comes to body modeling, from an algorithmic stand-point, you could want to cluster based on commonly co-occuring body features, but (binary) gender is just a bad (socially-constructed) proxy for that. Also, proxies are a thing in algo.~fairness research \cite{BigDataImpact}. I am beig told that this is apparently similar to Judith Butler's view on binary bodies in Gender Trouble?}

An analysis of the above reveals worrying trends about the current use of gender
as a variable in Computer Graphics, both scientifically and ethically. As we
mention examples of works that perpetuate these trends, we stress that we do not
associate any malicious intent to any. Rather, we wish to show how seemingly
neutral, well-established practices in our community can lead to us unwittingly
perpetuating forms of algorithmic bias.

%\ana{There is something to be said about the entire ``It's just the data thats biased'' discussion we keep having in ML ad nauseam. In our examples, it's not just the data that's the root of algorithmic unfairness, it's the active decisions of algorithm designers. For practitioners this means that, even if you don't have access to, e.g.~body scans of non-binary people, you can still try to remove a part of that bias that is due to algorithm design decisions.}

\section{Algorithmic Fairness Analysis}

Our literature survey demonstrates that the current use of gender in the computer graphics literature is at best ill-defined, and at worst incorrect. In this Section, we demonstrate through various examples why such an approach causes a number of \emph{technical} issues within the surveyed works, and is, as such, at odds with producing precise and high-quality reproducible research.

In our discussion, we apply a framework which disambiguates and categorizes different types of bias according to the stages of a system's lifecycle \cite{Suresh2021}. Note that while the original paper focuses on machine learning, we find it to be equally applicable to general problems in computer graphics. We give concrete examples of how \emph{all} types of bias occur throughout the surveyed work.

% \subsection{Task Definition and Data Unfairness}
\silvia{to-do}

\paragraph*{Historical bias} Historical bias occurs when data encodes existing prejudice. For example, a \emph{gender classifier} trained on data collected in a society where social norms dictate gender expression might learn that ``wearing a dress'' means woman, and ``short hair'' means man, despite the data being \emph{abundant} and \emph{perfectly sampled}. 

% \ana{Another possible example from Silvia: virtual garment try-on methods can merge a person's body proportions with their preference in attire. Not sure which is better to use?}

\paragraph*{Representation bias} This type of bias occurs when a part of a population is poorly represented by a dataset. This can happen due to a multitude of reasons:
\begin{itemize}
    \item \emph{Sample selection bias} occurs when the sampling procedure is biased in such a way to not include non-binary people. Given the scale of the works reviewed and the reported statistics about the prevalence of such individuals, the works we reviewed should include non-binary individuals; however, we could not identify a single paper that explicitly mentioned them in their motion capture actors, datasets or user study participants. A possible explanation is that the sampling procedure was accidentally designed in such a way to to decrease the likelihood of capturing gender-diverse individuals, or it might also occur due to measurement bias.
    \item A dataset can lead to algorithmic unfairness if it is uniformly sampled, but \emph{contains under-represented groups}. A uniformly sampled dataset with $1000$ people is expected include between $1$ and $20$ intersex and gender non-conforming individuals. This means that the algorithms trained on this dataset are more likely to produce worse results on these individuals than in the general population. In our survey, we did not identify a single paper that acknowledged the existence of intersex and gender non-conforming people; consequently, we did not identify a single paper that explicitly attempted to correct this type of representation bias. 
\end{itemize}

% Measurement, or reporting, bias arises from how we choose, utilize,and measure particular features.

\paragraph*{Measurement bias} Measurement, or reporting bias is typically introduced during the task definition through the selection and measurement of features and target variables. During our literature review, we identified the presence of the following issues:
\begin{itemize}
    \item The frequent usage of \emph{proxies}, both as features and as target variables. For example, we observed works that use \emph{sex} or \emph{gender} to mean \emph{comonly co-occuring bodily characteristics} or \emph{a set of voice attributes}.
    The use of \emph{sex} and \emph{gender} as proxy variables is not only harmful, but often also imprecise and inaccurate from a technical stand-point. It is quite possible that the authors would be better served using other less abstract features (e.g. hair length, or voice pitch). The imprecision can compound further when a single work combines several understandings of sex and gender; for example, we observed virtual try-on algorithms use gender both as a proxy for body parameters \emph{and} cultural choices in attire, and conversational agents use it to denote both the pitch of the agent's voice as well as cultural traits in their verbal and non-verbal communication. The usage of these variables can further lead to \emph{omitted variable bias}, which we discuss later.
    \item An \emph{inaccurate} method of measurement. For example, all of the papers we reviewed treat gender as a \emph{discrete binary variable}. Therefore, even if data was collected from gender-diverse participants, the method of measurement would not be able to capture that.
    \item An \emph{incorrect} method of measurement. We identified works that both propose and use gender labeling of image, voice and body geometry data, in many cases by automated systems or manual third parties as opposed to participant self-identification. Similarly, we observed works that report many user study participants' gender as "unknown", which may mean it is the researchers themselves who are attempting to assume their participant's gender without asking them to self-identify it. Algorithmic bias occurs in this situation, even if special care is taken to collect data of gender non-conforming individuals.
\end{itemize}.

\paragraph*{Aggregation bias}\silvia{to-do} \ana{I need help on this one, since I cannot think of any examples. From the survey paper: ``Aggregation bias (or ecological fallacy) arises when false conclusions are drawn about individuals from observing the entire population. An example of this type of bias can be seen in clinical aid tools. Consider diabetes patients who have apparent morbidity differences across ethnicities and genders.
Specifically, HbA1c levels, that are widely used to diagnose and monitor diabetes, differ in complex ways across genders and ethnicities. Therefore, a model that ignores individual differences will likely not be well-suited for all ethnic and gender groups in the population.
This is true even when they are represented equally in the training data. Any general assumptions about subgroups within the population can result in aggregation bias.''}

\paragraph*{Learning bias} \silvia{to-do}Algorithm designers will often unknowingly introduce bias into their methods. For example, using a regularizer that makes sure to regress to an ``average'' body is likely to make the algorithm worse for people far away from the average. Alternatively, optimizing for a model's accuracy might lead to the statistical pairity decreasing accross different groups.

\paragraph*{Evaluation bias} \silvia{to-do}Evaluation bias encompases all bias that is introduced during the evaluation of an algorithm. If the computer graphics community settles on benchmarks with biased data or metrics, it creates a knock-on effect where the development and deployment of models that performed well on those benchmarks is further encouraged. This is one of the reasons it is particularly dangerous to "leave the fairness questions" for future work.

\paragraph*{Deployment bias}\silvia{to-do} Deployment bias refers to the harm that is introduced as a consequence of the model being published or deployed in the real world:
\begin{itemize}
    \item Assuming an implicit definition of (e.g. binary) gender might incentivize future researchers to conform to that definition. This is additionally problematic when researchers from cultures with a different understanding of gender need to adjust to foreign cultural norms. (\ana{Amanda, I might need some help with this one.})
    \item Deploying an algorithm can lead to \emph{feedback loops}. For example, if gender-diverse people have poor experiences with clothing size recommender systems, they are less likely to use them. As a consequence, the data about the performance of such a system will be skewed to include fewer gender-diverse people. Using such data to further optimize the systems can lead to compounding effects.
    \item A deployed system can cause harm by nudging the users to artificially change their behavior. For example, a trans person might feel the need to change the pitch of their voice in order to not get misgendered by an algorithm.
    \item Deploying a body model which has ``male'', ``female'', and ``gender neutral'' variants might lead to unintentional harm, if, for example, the ``gender neutral'' model is used only that if the algorithm does not have enough confidence the user is either ``male'' or ``female''. This can lead to gender trans or non-confirming individuals being actiely told that they are not recognized as the gender they identify with.
\end{itemize}

\paragraph*{Omitted variable bias} Omitted vairable bias occurs when the success of using a certain feature is overemphasized because it correlates with another important feature that has been omitted from the modeling stage. For example, ``gender'' is likely not as discriminative as a variable when the result is also conditioned on ``hair length'', or ``hip width''. Alternatively, a voice modification algorithm may attribute its performance to gender, rather than a combination of pitch and certain socially-acquired speech inflections. \ana{cite paper that introduces this concept because it's not \cite{Suresh2021}.}

\ana{I don't know where to fit this: a proposed new conversational agent might learn a correlation between a person's visual appearance and certain traits they exhibit in non-verbal communication. Suggestions?}

In general, we have found the gender-related language to be imprecise, which hinders the clarity of the presentation. Oftentimes, we found that \emph{gender} and \emph{sex} are used seemingly interchangebly, and most of the times it is not even clear from context which one the authors intended to use. In fact, we argue that it is impossible to know a-priori if a trained model has picked up on \emph{gender} characteristics, and not on the characteristics of \emph{sex} or \emph{gender expression}, without looking at algorithmic fairness metrics accross different subgroups.

% society tends to conflate gender expression and gender identity.

A worrying trend we noticed is that none of the reviewed papers provide an analysis of the algorithmic biases that they potentially introduce. While different real-world constraints might not make it realistic for a research group to successfully mitigate certain sources of bias, the potentially introduced biases should at the very least be acknowledged. For example, none of the reviewed papers included any of the various algorithmic fairness metrics (\ana{cite metrics}) into their evaluation, nor did they even include a discussion of the potential harm their methods could be causing.

Algorithmic fairness evaluations and discussions are left out of computer graphics papers not because they happen to be difficult or time-consuming, but because they are deemed unnecessary by the people who would at large be either unaffected or positively affected by the introduced biases.

However, we argue that the problems introduced in these methods are not only potentially harmful to under-represented populations, but also often \emph{technically limited ways which are not well researched}. If a method cannot model a class of humans by design, or if a production system fails for a subsection of the population, these are fundamental \emph{technical} limitations. The question is then, why does our community prioritize solving these limitations disproportionatly less then other technical problems?

\subsection{Real world harm}

The discussed technical limitations of the reviewed algorithms are not only academic in nature, but can and do lead to real world harm. The algorithmic fairness literature disambiguates between allocative and representational harms \ana{cite}.

Representational harms encompas the perpetuation of harmful stereotypes or cultural norms that subject individuals to denegration. For example, it is well documented that airport body scanners routinely subject transgender passangers to public humilliation \cite{tsa}.

On the other hand, allocative harms are caused when a certain groups are denied access to an opportunity or a resource because of algorithmic bias. For example, a virtual try-on experience  might accidentally exclude precisely the people with non-normative bodies who are most in danger in traditional physical changing rooms \cite{changingroom}.

Furthermore, computer graphics is being increasingly applied to other fields, such as for processing geometric data in medicine, or for synthetic dataset generation in computer vision with numerous downstream applications \cite{cars, chen2021synthetic, dhs}. Therefore, it is paramount to understand that our algorithms can and will be used in novel ways potentially causing harm in ways we did not intend.

Finally, the current use of gender in the Computer Graphics literature creates an alianating and exclusionary environment for gender-diverse members of our research community, going directly against ACM SIGGRAPH's goal to be \emph{a model of inclusion, equity, access and diversity for all}.

\section{Where do we go from here?}

\ana{Some suggestions for this section:
\begin{itemize}
    \item Algorithmic fairness should not be an afterthought, but something we plan for from the beginning, as it can be introduced in every step of algorithm development.
    \item Data collection should account for how much representation is necessary from each marginalized group.
    \item Algorithmic fairness metrics exist and are easy to implement. Use them, and report them in your research.
    \item In our review, we found that gender or sex could have often been replaced by another variable and it would have been more accurate.
    \item We argue that discussions around algorithmic fairness need to become front-and-center within our own community, instead of being relegated to other venues or "future work".
\end{itemize}}

\ana{This paragraph might be better suited for this section: It bears mentioning that our research community's entrenchment in the
traditional gender binary is a rare example of Computer Graphics research
lagging behind the needs of our partner industries. \emph{Metahuman}, the latest
photorrealistic character modeller by \citet{metahuman} has no mention of
gender; \citet{googlegender} removed all gender references from its Cloud Vision
API; video games as diverse as \emph{Animal Crossing: New Horizons},and \emph{Forza Horizon 5} completely decouple attributes
like hairstyle, body proportions, voice pitch and prononouns from one another.}

\ana{This might also be better for the conclusion: As Computer Graphics researchers, we must consider our role in shaping whose stories get to be told and who gets to seem themselves represented in the entertainment culture.}

We believe the reasons above to be enough to make us reevaluate the role of
gender in our community's scientific literature.

For example, the reporting of gender among other demographic information in user
study participants and dataset collection subjects answer to a scientifically
positive goal (experimental transparency) as well as an ethical one, to
safeguard against the ‚Äúmale default‚Äù that plagues science and has plagued it
since its infancy. However, we found instances in our survey of participants
being reported as of ‚Äúunknown gender‚Äù, which may indicate that their gender is
being assumed post facto by researchers as opposed to self reported, leading to
the potential misidentification and exclusion of gender non-conforming
individuals or of those from certain ethnicities (see e.g.,
\cite{santamaria2018comparison,buolamwini2018gender}). Therefore, we would argue
it is still advisable to include this kind of data, as long as it is self
reported by participants who are given a breadth of gender options not
restricted to the traditional binary ones.

On the other hand, the scientific and ethical harm caused by gender-segregated
algorithms is likely too significant to offset any possible benefits. At the
very least, these choices should be justified and their consequences in terms of
excluding gender non-conforming individuals should be examined and clearly
stated. Eventually, we hope that our field evolves to address these limitations
and move beyond the outdated gender binary. We trust that our fellow researchers
share our scientific excitement in this new frame of reference and the potential
novel research directions it opens; for example:
\begin{itemize}
    \item What is a complete parametric model for the human body that is
    decoupled from gender and accurately represents the diverse bodies of all
    humans, regardless of whether they conform to traditional gender norms?
    \item How can our research inform or contrast more modern understandings of
    gender? Can data-based methods be used to evaluate cultural differences in
    gender presentation?
    \item How can we evaluate our algorithms for bias towards the gender binary?
    What tools are needed to obtain or synthesize data that covers more diverse
    experiences of gender?
\end{itemize}

We acknowledge that our proposed break with tradition may bring with it effort
and difficult conversations, but these are challenges worth facing in the
interest of scientific advancement as well as producing a fairer, more inclusive
future.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

\end{document}
