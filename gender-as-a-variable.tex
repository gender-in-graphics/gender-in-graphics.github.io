\PassOptionsToPackage{table}{xcolor}
\documentclass[sigconf,balance=false]{acmart}
\citestyle{acmauthoryear}
\setcitestyle{square}

\setcopyright{rightsretained} 
\copyrightyear{2022} 
\acmYear{2022} 
\acmConference{SIGGRAPH '22 Talks}{August 07-11, 2022}{Vancouver, BC, Canada}\acmBooktitle{Special Interest Group on Computer Graphics and Interactive Techniques Conference Talks (SIGGRAPH '22 Talks), August 07-11, 2022}\acmDOI{10.1145/3532836.3536227}
\acmISBN{978-1-4503-9371-3/22/08}

\settopmatter{authorsperrow=4}
\input{common-preamble}

\begin{document}

\title{Sex and Gender in the Computer Graphics Research Literature}

\author{Ana Dodik}\authornote{Joint First Authors}\authornote{Work done while the author was employed by Meta Platforms, Inc.}
\affiliation{\institution{MIT}\country{United States of America}}
\email{anadodik@mit.edu}

\author{Silvia Sell√°n}\authornotemark[1]
\affiliation{\institution{University of Toronto}\country{Canada}}
\email{sgsellan@cs.toronto.edu}

\author{Theodore Kim}
\affiliation{\institution{Yale University}\country{United States of America}}\email{theodore.kim@yale.edu}

\author{Amanda Phillips}
\affiliation{\institution{Georgetown University}\country{United States of America}}\email{amanda.phillips@georgetown.edu}



\begin{abstract}
    We survey the treatment of sex and gender in the Computer Graphics research
    literature from an algorithmic fairness perspective. The
    stablished practices on the use of gender and sex in our community are scientifically incorrect and constitute a
    form of algorithmic bias with potential harmful effects. We propose ways of addressing these as technical limitations.
\end{abstract}

%   Mention good examples

\maketitle

%\ana{I understand wanting to focus on SIGGRAPH, but I think we should consider broadening the scope to include all of visual computing research. Computer vision is \textit{especially} guilty of this.}
\section{Introduction}

%\ted{this is a sample Ted Kim comment}

Sex and gender are referenced in the Computer Graphics literature: a dataset is said to contain images of men and women, user study participants are reported with certain male/female ratios, a body modeling algorithm trains two different gendered models, a voice modification method is said to work on male and female voices, etc.

% also mention society, maybe industry as fields moving away from this binary

% Amanda comments:

% flip-flop between sex and gender: commit to one or other or both. (So far committed to both)

% be clear the critique is about binarism, not the use of gender itself

% Claire Hemmings: cite journals and not person

% more specific examples in the text

The scientific consensus around sex and gender has evolved in the past decades \cite{pmid30377332}. As surveyed by \citet{fausto2012sex}, \emph{sex} is not one but a combination of many biological classifications (\emph{chromosomal, hormonal, reproductive, ...}) which cannot be assigned in a binary way to as many as one in 50 people \cite{blackless2000sexually}.
\emph{Gender} refers to an individual's self-identity \cite{money1972man}, or their performance as shaped by social expectations \cite{butler2003gender}. It also commonly refers to arbitrary organizational structures that segregate people into, e.g. different public bathrooms or professions \cite{lorber1994paradoxes}. In these contemporary definitions, gender is fluid, culturally-specific and not binary. Assuming outdated binary definitions of sex and gender is not just scientifically incorrect, but is also harmful to those who conform the least to this artificial binary (e.g, \emph{intersex}, \emph{transgender}, \emph{non-binary} people), whom we call \emph{gender non-conforming} \cite{un2015report}.

The treatment of sex and gender in SIGGRAPH Technical Papers
still adheres to a traditional binary understanding, excluding intersex, transgender, and
gender non-conforming people. Furthermore, it makes research lag behind the needs of industry. The latest character modeller for \citet{metahuman} and the Cloud Vision API by \citet{googlegender} have removed references to sex and gender. \emph{Animal Crossing} and \emph{Forza Horizon} completely decouple attributes like body proportions, voice pitch, hairstyle and pronouns. 

We will use an algorithmic fairness lens to argue that this binary understanding adds algorithmic biases detrimental to scientific integrity.
We will examine the real-world harms caused these biases in how gender non-conforming people interact with
our technology. We advocate for a reexamination of our treatment of gender, and show that correcting problematic practices in
our community will open the door to new avenues of research.

\section{Survey}

%\ana{So a small suggestion on how to make this more formal, we could indicate which type of algorithmic unfairness the different papers introduce, e.g.~\cite{FriedmanAndNissenbaum}.}

Inspired by \citet{keyes2018misgendering}, we survey
all technical papers presented at any SIGGRAPH since 2015. We list all 64 containing mentions of sex or gender in our \textbf{Supplemental Material}, along with our main observations (O1-7). We make the deliberate choice to reference these observations and not specific works in this main text to stress that we do not associate any malicious intent to individual authors. Rather, we are showing how seemingly neutral, well-established practices in our community (which includes journals, editors, reviewers, etc.) can unwittingly perpetuate forms of algorithmic bias.

Our observed references to sex and gender varied in nature from demographic information regarding study participants (\userstudy) or dataset makeup (\dataset) to gender-specific algorithms (\var). Whenever gender or sex is used
explicitly as a variable, it is always a binary (\binary) proxy for features such as body proportions or speech characteristics. The existence of gender non-conforming people was never acknowledged (O3). We found works proposing or using image-based (binary) gender recognition algorithms (\classifier).

%From an algorithmic fairness perspective (\S\ref{sec:analysis}), our analysis reveals a worrying status quo in the use of gender and sex
%as variables. When we
%mention specific examples that perpetuate these trends, we stress that we are not
%associating any malicious intent. Rather, we are showing how seemingly
%neutral, well-established practices can unwittingly
%perpetuating forms of algorithmic bias.

%\ana{There is something to be said about the entire ``It's just the data thats biased'' discussion we keep having in ML ad nauseam. In our examples, it's not just the data that's the root of algorithmic unfairness, it's the active decisions of algorithm designers. For practitioners this means that, even if you don't have access to, e.g.~body scans of non-binary people, you can still try to remove a part of that bias that is due to algorithm design decisions.}

\subsection{Algorithmic Fairness Analysis}\label{sec:analysis}

Our survey shows that the current use of gender and sex in Computer Graphics is at best ill-defined, and at worst incorrect.
We apply the framework of \citet{Suresh2021}, which categorizes bias according to the stages of a system's lifecycle 
(see also \cite{fairnesssurvey,FriedmanAndNissenbaum,olteanu2019social}).
We give examples of how different types of bias occur, and show that these are \emph{technical} limitations that impede the development of precise, high-quality, reproducible research.

\paragraphtight{Representation bias} Portions of populations may be poorly represented by a dataset, e.g., because the sampling procedure did not include people of non-binary genders (\emph{sample selection bias}) or because algorithm performance was not evaluated on groups of \emph{underrepresented} sex or gender. Despite the prevalence of these individuals in the general population, we did not identified a single paper (O3) that explicitly mentioned them as part of datasets (\dataset) or user study participants (\userstudy). The sampling procedure may have been unintentionally designed to exclude these individuals, or it might be due to measurement bias. We did not identify any work that explicitly analyzed any type of representation bias experienced by gender non-conforming individuals (\binary).
% \begin{itemize}
%     \item \emph{Sample selection bias} occurs when the sampling procedure is biased not include non-binary people. Given the scale of the works reviewed and the reported statistics about the prevalence of such individuals, the works we reviewed should include non-binary individuals; however, we could not identify a single paper that explicitly mentioned them as part of datasets or user study participants. The sampling procedure may have been accidentally designed to exclude these individuals, or it might also be due to measurement bias.
%     \item A dataset can lead to algorithmic unfairness if it is uniformly sampled, but \emph{contains under-represented groups}. A uniformly sampled dataset with $1000$ people is expected include between $1$ and $20$ intersex or gender non-conforming individuals. This means that the algorithms trained on this dataset are more likely to produce worse results on these individuals than in the general population. In our survey, we did not identify a single paper that acknowledged the existence of intersex and gender non-conforming people; consequently, we did not identify a single paper that explicitly attempted to correct this type of representation bias.
% \end{itemize}

\paragraphtight{Historical bias} Data, despite being abundant and perfectly sampled, may encode existing prejudice. For example, a \emph{gender classifier} (\classifier) trained on portrait image data collected in an environment where social norms dictate gender expression might learn that ``wearing a dress'' means woman, and ``short hair'' means man.

\paragraphtight{Measurement bias} Bias may be introduced through the selection and measurement of features and target variables.
Many works use sex or gender as imprecise \emph{proxies} (\var) for attributes like \emph{commonly co-occuring bodily} or \emph{speech characteristics}, in lieu of less abstract features like hair length or voice pitch. Some works even combined proxies, e.g., conversational agents that use gender for voice pitch \emph{and} culturally acquired speech inflections.
%virtual try-on algorithms that use gender both as a proxy for body geometry \emph{and} cultural choices in attire.

When gender or sex was chosen as a feature or target variable, it was always (\binary) through an \emph{inaccurate method of measurement}, such as treating gender as a binary variable that excludes non-binary individuals by design. Alternatively (\classifier) \emph{incorrect methods of measurement} were used, such as image-based gender classifiers in lieu of self-identification, which can cause gender non-conforming individuals to be misidentified.

\paragraphtight{Omitted variable bias} A successful feature may correlate with an important feature that has been omitted from the model (see e.g., \cite{clarke2005phantom}). \emph{Gender} or \emph{sex} are likely not as discriminative when the result is also conditioned on \emph{hair length}, \emph{hip width} or \emph{mean voice frequency}. When the use of gender or sex was justified because of an assumed improvement in accuracy (\var), we found no attempt to identify if the success was due to omitted variables.

% During our literature review, we identified the presence of the following issues:
% \begin{itemize}
%     \item The frequent usage of \emph{proxies}, both as features and as target variables. For example, we observed works that use \emph{sex} or \emph{gender} to mean \emph{comonly co-occuring bodily characteristics} or \emph{a set of voice attributes}.
%     The use of \emph{sex} and \emph{gender} as proxy variables is not only harmful, but often also imprecise and inaccurate from a technical stand-point. It is quite possible that the authors would be better served using other less abstract features (e.g. hair length, or voice pitch). The imprecision can compound further when a single work combines several understandings of sex and gender; for example, we observed virtual try-on algorithms use gender both as a proxy for body parameters \emph{and} cultural choices in attire, and conversational agents use it to denote both the pitch of the agent's voice as well as cultural traits in their verbal and non-verbal communication. The usage of these variables can further lead to \emph{omitted variable bias}, which we discuss later.
%     \item An \emph{inaccurate} method of measurement. For example, all of the papers we reviewed treat gender as a \emph{discrete binary variable}. Therefore, even if data was collected from gender-diverse participants, the method of measurement would not be able to capture that.
%     \item An \emph{incorrect} method of measurement. We identified works that both propose and use gender labeling of image, voice and body geometry data, in many cases by automated systems or manual third parties as opposed to participant self-identification. Similarly, we observed works that report many user study participants' gender as "unknown", which may mean it is the researchers themselves who are attempting to assume their participant's gender without asking them to self-identify it. Algorithmic bias occurs in this situation, even if special care is taken to collect data of gender non-conforming individuals.
% \end{itemize}.



%\paragraphtight{Aggregation bias}\silvia{to-do} \ana{I need help on this one, since I cannot think of any examples. From the survey paper: ``Aggregation bias (or ecological fallacy) arises when false conclusions are drawn about individuals from observing the entire population. An example of this type of bias can be seen in clinical aid tools. Consider diabetes patients who have apparent morbidity differences across ethnicities and genders.
%Specifically, HbA1c levels, that are widely used to diagnose and monitor diabetes, differ in complex ways across genders and ethnicities. Therefore, a model that ignores individual differences will likely not be well-suited for all ethnic and gender groups in the population.
% This is true even when they are represented equally in the training data. Any general assumptions about subgroups within the population can result in aggregation bias.''}

%\paragraphtight{Learning bias} \silvia{to-do}Algorithm designers will often unknowingly introduce bias into their methods. For example, using a regularizer that makes sure to regress to an ``average'' body is likely to make the algorithm worse for people far away from the average. Alternatively, optimizing for a model's accuracy might lead to the statistical pairity decreasing accross different groups.

\paragraphtight{Evaluation bias} These are biases occur during evaluation of an algorithm, such as body modeling works that provide binary segregated parametric models (O3). These are then used to evaluate \emph{other} works with orthogonal contributions, like virtual try-on or motion capture. If our community codifies biased benchmarks, we encourage the development of models that conform to those biases.

%\silvia{to-do}Evaluation bias encompases all bias that is introduced during the evaluation of an algorithm. If the computer graphics community settles on benchmarks with biased data or metrics, it creates a knock-on effect where the development and deployment of models that performed well on those benchmarks is further encouraged. This is one of the reasons it is particularly dangerous to "leave the fairness questions" for future work.

% \subsection{Real world harm}

% Unlike our survey, the effects of algorithmic bias do not stop at the publication stage. Rather, harm is introduced as a consequence of a model being used in the real world, known as \emph{deployment bias}.

\paragraphtight{Deployment bias} Real-world harm is introduced when graphics models are published or deployed. The exclusive publication of papers with a binary understanding of sex and gender incentivizes researchers (and reviewers) to conform to that definition (\binary).
This leads to \emph{feedback loops}: if gender non-conforming people are not included in a virtual clothing try-on system, they are less likely to use it, skewing the system's performance data to include them even less. Finally, a system can impose its biases onto user behavior: a trans person may need to change the pitch of their voice in order to not get misgendered by an algorithm, further skewing the data.

\subsection{Real world harm}

The technical limitations of the reviewed algorithms can lead to real world harms. As Computer Graphics is increasingly applied to other fields, such as geometric data processing in medicine, or for synthetic dataset generation in computer vision, with numerous downstream applications \cite{cars, chen2021synthetic, dhs}, it is paramount to understand that our algorithms can and will be used in novel ways that can cause unintended harms. The algorithmic fairness literature disambiguates between {\em representational} and {\em allocative} harms \cite{barocas-hardt-narayanan}.

{\bf Representational harms} encompass the perpetuation of stereotypes or cultural norms that subject individuals to denigration. For example, airport body scanners routinely subject gender non-conforming passengers to public humiliation \cite{tsa}.

{\bf Allocative harms} are when certain groups are denied access to a resource because of algorithmic bias. For example, a virtual try-on experience based on biased algorithms might exclude the precise people with non-normative bodies who are most in danger in traditional physical changing rooms \cite{changingroom}.

Finally, ignoring the existence of gender non-conforming individuals in our research (O3) creates an alienating and exclusionary environment for these exact members of our research community, directly contravening SIGGRAPH's goal to be \emph{a model of inclusion, equity, access and diversity for all}.

\section{Where do we go from here?}

Our analysis reveals that the common use of sex and gender in Computer Graphics can pepper our research with algorithmic bias. Our disambiguated study shows bias throughout the modeling process: algorithmic fairness cannot be an afterthought but must present at all stages of our research. We have focused on sex and gender, but hope our work broadens conversations about algorithmic fairness.% in our field, including racial biases \cite{kim2021countering}.

Various real-world constraints may make it unrealistic for specific research groups to mitigate certain sources of bias, but potentially introduced biases should still be acknowledged. For example, none of the surveyed papers included algorithmic fairness metrics (for a summary, see \cite{fairnesssurvey, fairnessmetrics}) in their evaluation, nor did they include a discussion of how their treatment of sex and gender could cause potential harm.

The issues raised by our survey often reveal \emph{scientific} limitations. If a method cannot model a class of humans, or a production system fails for a subsection of the population, these are fundamental \emph{technical} limitations, and should be discussed as such. Gender and sex can have a place in our research. It would be beneficial to report them among demographic statistics of datasets or user study participants (self-reported and non binary, in agreement with the scientific consensus) to
safeguard against the ‚Äúmale default‚Äù that plagues the sciences. In most cases where we observed sex or gender being used as features or targets, they should have been replaced by other, more accurate, variables. Finding these omitted variables and disaggregating the attributes that have been traditionally crammed into sex and gender constitute important open research problems.

Our proposed break with tradition requires effort, and difficult conversations. These are challenges worth facing if we want scientific advances to produce a fairer, more inclusive future.
% \begin{itemize}
%     \item Assuming an implicit definition of (e.g. binary) gender might incentivize future researchers to conform to that definition. This is additionally problematic when researchers from cultures with a different understanding of gender need to adjust to foreign cultural norms. (\ana{Amanda, I might need some help with this one.})
%     \item Deploying an algorithm can lead to \emph{feedback loops}. For example, if gender-diverse people have poor experiences with clothing size recommender systems, they are less likely to use them. As a consequence, the data about the performance of such a system will be skewed to include fewer gender-diverse people. Using such data to further optimize the systems can lead to compounding effects.
%     \item A deployed system can cause harm by nudging the users to artificially change their behavior. For example, a trans person might feel the need to change the pitch of their voice in order to not get misgendered by an algorithm.
%     \item Deploying a body model which has ``male'', ``female'', and ``gender neutral'' variants might lead to unintentional harm, if, for example, the ``gender neutral'' model is used only that if the algorithm does not have enough confidence the user is either ``male'' or ``female''. This can lead to gender trans or non-confirming individuals being actiely told that they are not recognized as the gender they identify with.
% \end{itemize}



%\ana{I don't know where to fit this: a proposed new conversational agent might learn a correlation between a person's visual appearance and certain traits they exhibit in non-verbal communication. Suggestions?}

%\silvia{to-do}

% In general, we have found the gender-related language to be imprecise, which hinders the clarity of the presentation. Oftentimes, we found that \emph{gender} and \emph{sex} are used seemingly interchangebly, and most of the times it is not even clear from context which one the authors intended to use. In fact, we argue that it is impossible to know a-priori if a trained model has picked up on \emph{gender} characteristics, and not on the characteristics of \emph{sex} or \emph{gender expression}, without looking at algorithmic fairness metrics accross different subgroups.

% society tends to conflate gender expression and gender identity.

% A worrying trend we noticed is that none of the reviewed papers provide an analysis of the algorithmic biases that they potentially introduce. While different real-world constraints might not make it realistic for a research group to successfully mitigate certain sources of bias, the potentially introduced biases should at the very least be acknowledged. For example, none of the reviewed papers included any of the various algorithmic fairness metrics (for a summaries, see \cite{fairnesssurvey, fairnessmetrics}) into their evaluation, nor did they even include a discussion of the potential harm their methods could be causing.

% Algorithmic fairness evaluations and discussions are left out of computer graphics papers not because they happen to be difficult or time-consuming, but because they are deemed unnecessary by the people who would at large be either unaffected or positively affected by the introduced biases.

% However, we argue that the problems introduced in these methods are not only potentially harmful to under-represented populations, but also often \emph{technically limited ways which are not well researched}. If a method cannot model a class of humans by design, or if a production system fails for a subsection of the population, these are fundamental \emph{technical} limitations. The question is then, why does our community prioritize solving these limitations disproportionatly less then other technical problems?


% \ana{Some suggestions for this section:
% \begin{itemize}
%     \item Algorithmic fairness should not be an afterthought, but something we plan for from the beginning, as it can be introduced in every step of algorithm development.
%     \item Data collection should account for how much representation is necessary from each marginalized group.
%     \item Algorithmic fairness metrics exist and are easy to implement. Use them, and report them in your research.
%     \item In our review, we found that gender or sex could have often been replaced by another variable and it would have been more accurate.
%     \item We argue that discussions around algorithmic fairness need to become front-and-center within our own community, instead of being relegated to other venues or "future work".
% \end{itemize}}

% \ana{This paragraph might be better suited for this section: It bears mentioning that our research community's entrenchment in the
% traditional gender binary is a rare example of Computer Graphics research
% lagging behind the needs of our partner industries. \emph{Metahuman}, the latest
% photorrealistic character modeller by \citet{metahuman} has no mention of
% gender; \citet{googlegender} removed all gender references from its Cloud Vision
% API; video games as diverse as \emph{Animal Crossing: New Horizons},and \emph{Forza Horizon 5} completely decouple attributes
% like hairstyle, body proportions, voice pitch and prononouns from one another.}

% \ana{This might also be better for the conclusion: As Computer Graphics researchers, we must consider our role in shaping whose stories get to be told and who gets to seem themselves represented in the entertainment culture.}

% We believe the reasons above to be enough to make us reevaluate the role of
% gender in our community's scientific literature.

% For example, the reporting of gender among other demographic information in user
% study participants and dataset collection subjects answer to a scientifically
% positive goal (experimental transparency) as well as an ethical one, to
% safeguard against the ‚Äúmale default‚Äù that plagues science and has plagued it
% since its infancy. However, we found instances in our survey of participants
% being reported as of ‚Äúunknown gender‚Äù, which may indicate that their gender is
% being assumed post facto by researchers as opposed to self reported, leading to
% the potential misidentification and exclusion of gender non-conforming
% individuals or of those from certain ethnicities (see e.g.,
% \cite{santamaria2018comparison,buolamwini2018gender}). Therefore, we would argue
% it is still advisable to include this kind of data, as long as it is self
% reported by participants who are given a breadth of gender options not
% restricted to the traditional binary ones.

% On the other hand, the scientific and ethical harm caused by gender-segregated
% algorithms is likely too significant to offset any possible benefits. At the
% very least, these choices should be justified and their consequences in terms of
% excluding gender non-conforming individuals should be examined and clearly
% stated. Eventually, we hope that our field evolves to address these limitations
% and move beyond the outdated gender binary. We trust that our fellow researchers
% share our scientific excitement in this new frame of reference and the potential
% novel research directions it opens; for example:
% \begin{itemize}
%     \item What is a complete parametric model for the human body that is
%     decoupled from gender and accurately represents the diverse bodies of all
%     humans, regardless of whether they conform to traditional gender norms?
%     \item How can our research inform or contrast more modern understandings of
%     gender? Can data-based methods be used to evaluate cultural differences in
%     gender presentation?
%     \item How can we evaluate our algorithms for bias towards the gender binary?
%     What tools are needed to obtain or synthesize data that covers more diverse
%     experiences of gender?
% \end{itemize}

% We acknowledge that our proposed break with tradition may bring with it effort
% and difficult conversations, but these are challenges worth facing in the
% interest of scientific advancement as well as producing a fairer, more inclusive
% future.

\bibliographystyle{ACM-Reference-Format}
\bibliography{references.bib}

\end{document}
